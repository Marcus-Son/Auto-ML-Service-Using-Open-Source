import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, roc_curve, auc,
    roc_auc_score, precision_recall_curve
)

try:
    import shap
except ImportError:
    shap = None

def classification_metrics(y_true, y_pred):
    """ê¸°ë³¸ ë¶„ë¥˜ ì„±ëŠ¥ì§€í‘œ dict ë¦¬í„´"""
    metrics = {
        "Accuracy": accuracy_score(y_true, y_pred),
        "F1": f1_score(y_true, y_pred, average="weighted"),
        "Precision": precision_score(y_true, y_pred, average="weighted", zero_division=0),
        "Recall": recall_score(y_true, y_pred, average="weighted", zero_division=0)
    }
    # ROC AUC (ì´ì§„ ë¶„ë¥˜ë§Œ)
    if len(np.unique(y_true)) == 2:
        try:
            metrics["ROC_AUC"] = roc_auc_score(y_true, y_pred)
        except Exception:
            metrics["ROC_AUC"] = np.nan
    return metrics

def show_metrics(y_true, y_pred):
    st.subheader("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ (Test Set)")
    metrics = classification_metrics(y_true, y_pred)
    st.write({k: f"{v:.4f}" for k, v in metrics.items()})

    st.markdown("""
    - **Accuracy**: ì „ì²´ ì˜ˆì¸¡ ì •ë‹µ ë¹„ìœ¨ (ì§ê´€ì  ì§€í‘œ)
    - **F1**: ì •ë°€ë„-ì¬í˜„ìœ¨ ê· í˜•, ë¶ˆê· í˜•/ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©
    - **Precision/Recall**: í´ë˜ìŠ¤ë³„ ì˜¤íƒ/ë¯¸íƒ ë¯¼ê°
    - **ROC_AUC**: ì´ì§„ ë¶„ë¥˜ì—ì„œ ì˜ˆì¸¡ í™•ë¥ ì˜ ë¶„ë³„ë ¥ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
    """)

def plot_confusion_matrix(y_true, y_pred, labels=None):
    st.subheader("ğŸ”¢ Confusion Matrix")
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=labels if labels is not None else np.unique(y_true),
                yticklabels=labels if labels is not None else np.unique(y_true))
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    st.pyplot(fig)

def plot_roc_curve(y_true, y_score):
    st.subheader("ğŸ“ˆ ROC Curve")
    fpr, tpr, _ = roc_curve(y_true, y_score)
    auc_val = auc(fpr, tpr)
    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, label=f"AUC={auc_val:.2f}")
    ax.plot([0,1], [0,1], 'r--')
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("ROC Curve")
    ax.legend()
    st.pyplot(fig)

def plot_precision_recall_curve(y_true, y_score):
    st.subheader("ğŸ“ˆ Precision-Recall Curve")
    precision, recall, _ = precision_recall_curve(y_true, y_score)
    fig, ax = plt.subplots()
    ax.plot(recall, precision)
    ax.set_xlabel("Recall")
    ax.set_ylabel("Precision")
    ax.set_title("Precision-Recall Curve")
    st.pyplot(fig)

def plot_feature_importance(model, X_test):
    st.subheader("ğŸ§‘â€ğŸ”¬ Permutation Feature Importance (PFI)")
    try:
        from sklearn.inspection import permutation_importance
        result = permutation_importance(model, X_test, model.predict(X_test), n_repeats=20, random_state=42)
        importance = pd.Series(result.importances_mean, index=X_test.columns)
        topk = importance.abs().sort_values(ascending=False).head(15)
        fig, ax = plt.subplots()
        topk.plot.barh(ax=ax)
        ax.set_title("Permutation Feature Importance (ìƒìœ„ 15)")
        st.pyplot(fig)
    except Exception as e:
        st.warning(f"Permutation Importance ê³„ì‚° ì‹¤íŒ¨: {e}")

def plot_shap(model, X_test):
    st.subheader("ğŸ” SHAP Explainability")
    if shap is None:
        st.info("SHAP ì„¤ì¹˜ í•„ìš”: `pip install shap`")
        return
    try:
        explainer = shap.Explainer(model, X_test)
        shap_values = explainer(X_test)
        st.info("ìƒì—… AutoMLì—ì„œë„ ìì£¼ ì“°ì´ëŠ” ì „ì²´/ê°œë³„ í•´ì„")
        # ì „ì²´ summary plot
        fig1 = shap.plots.beeswarm(shap_values, show=False)
        st.pyplot(bbox_inches='tight', pad_inches=0)
        st.markdown("**SHAP summary plot:** ì „ì²´ feature ì˜í–¥ë ¥")
        st.pyplot(fig1)
        st.markdown("---")
        # ê°œë³„ force plot (ì²«ë²ˆì§¸ ìƒ˜í”Œ)
        idx = st.slider("ê°œë³„ ì˜ˆì¸¡ ìƒ˜í”Œ(index)", 0, X_test.shape[0]-1, 0)
        st.markdown("**ê°œë³„ ì˜ˆì¸¡ SHAP force plot**")
        fig2 = shap.plots.force(shap_values[idx], matplotlib=True, show=False)
        st.pyplot(fig2)
    except Exception as e:
        st.warning(f"SHAP ì„¤ëª… ì‹¤íŒ¨: {e}")

def plot_top_errors(y_true, y_pred, X_test, top_n=5):
    st.subheader(f"ì˜ˆì¸¡ ì˜¤ë¥˜(ì‹¤íŒ¨) Top {top_n}")
    errors = (y_true != y_pred)
    wrong_idx = np.where(errors)[0][:top_n]
    st.write(X_test.iloc[wrong_idx])
    st.write("ì‹¤ì œê°’:", pd.Series(y_true).iloc[wrong_idx].values)
    st.write("ì˜ˆì¸¡ê°’:", pd.Series(y_pred).iloc[wrong_idx].values)

def advanced_evaluation(model, X_test, y_test):
    y_pred = model.predict(X_test)
    show_metrics(y_test, y_pred)
    plot_confusion_matrix(y_test, y_pred)
    # ì´ì§„ ë¶„ë¥˜ì¼ ë•Œë§Œ ROC/PR ê³¡ì„ 
    if len(np.unique(y_test)) == 2:
        try:
            y_score = model.predict_proba(X_test)[:, 1]
            plot_roc_curve(y_test, y_score)
            plot_precision_recall_curve(y_test, y_score)
        except Exception:
            pass
    plot_feature_importance(model, X_test)
    plot_shap(model, X_test)
    plot_top_errors(y_test, y_pred, X_test)

    st.markdown("""
    ---
    ## ìƒì—… AutoML ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼
    - ì£¼ìš” ì„±ëŠ¥ì§€í‘œ ë° í•´ì„¤
    - Confusion Matrix/PR/ROC ê³¡ì„ 
    - Permutation/SHAP feature importance
    - ì˜ˆì¸¡ì‹¤íŒ¨ ìƒ˜í”Œ ìë™ í‘œì‹œ
    """)

def evaluate(model, X_test, y_test):
    """ìƒì—… AutoML ìŠ¤íƒ€ì¼ ë¶„ë¥˜ í‰ê°€+XAI"""
    advanced_evaluation(model, X_test, y_test)