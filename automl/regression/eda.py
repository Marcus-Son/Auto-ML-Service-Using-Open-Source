# automl/regression/eda.py
"""
ê³ ë„í™”ëœ Regression EDA
-----------------------
ê° EDA ì„¹ì…˜ë³„ë¡œ ê²°ê³¼ ê¸°ë°˜ ë™ì  í•´ì„¤(ìë™ í•´ì„) ì¶”ê°€
"""
from __future__ import annotations
import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

try:
    import ydata_profiling
except ImportError:
    ydata_profiling = None
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.neighbors import LocalOutlierFactor
except ImportError:
    IsolationForest = LocalOutlierFactor = None
try:
    from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
except ImportError:
    vif = None

_SAMPLE_SIZE = 10000

@st.cache_data(show_spinner=False)
def sample_df(df: pd.DataFrame, max_rows=_SAMPLE_SIZE) -> pd.DataFrame:
    if len(df) > max_rows:
        return df.sample(n=max_rows, random_state=42)
    return df.copy()

def _overview(df: pd.DataFrame):
    st.subheader("0ï¸âƒ£ ë°ì´í„° ê°œìš”")
    st.info("ë°ì´í„°ì˜ ì „ì²´ êµ¬ì¡°, ë³€ìˆ˜ ì¢…ë¥˜, ìš©ëŸ‰ì„ í•œëˆˆì— í™•ì¸í•©ë‹ˆë‹¤. ì´ìƒì¹˜/ê²°ì¸¡ íƒìƒ‰ì´ë‚˜ ì „ì²˜ë¦¬ ë²”ìœ„ íŒŒì•…ì— ì¤‘ìš”í•©ë‹ˆë‹¤.")
    col1, col2 = st.columns(2)
    with col1:
        st.write("í–‰/ì—´:", df.shape)
        st.write("ë°ì´í„° íƒ€ì…:", dict(df.dtypes.value_counts()))
        st.write("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:", f"{df.memory_usage(deep=True).sum()/1e6:.2f} MB")
    with col2:
        st.write(df.head(5))
    st.caption(f"ì´ ë°ì´í„°ëŠ” {df.shape[0]:,}ê°œì˜ í–‰ê³¼ {df.shape[1]}ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
               f"{', '.join([f'{k}: {v}' for k, v in dict(df.dtypes.value_counts()).items()])} íƒ€ì… ë³€ìˆ˜ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")

def _missing_and_duplicates(df: pd.DataFrame):
    st.subheader("1ï¸âƒ£ ê²°ì¸¡ & ì¤‘ë³µ")
    st.info("ê²°ì¸¡ì¹˜ëŠ” ì „ì²˜ë¦¬ ì „ëµ(ì‚­ì œ/ëŒ€ì¹˜) ì„¤ê³„, ì¤‘ë³µ ë°ì´í„°ëŠ” ëª¨ë¸ë§ í¸í–¥ ë°©ì§€ì— ê¼­ ì²´í¬í•´ì•¼ í•©ë‹ˆë‹¤.")
    miss = df.isna().mean().sort_values(ascending=False)
    dup_cnt = df.duplicated().sum()
    if miss.max() == 0:
        st.success("âœ… ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.caption("â†’ ì¶”ê°€ ì „ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        fig, ax = plt.subplots(figsize=(6, 3))
        miss[miss > 0].plot.bar(ax=ax)
        ax.set_ylabel("Missing Ratio")
        st.pyplot(fig, use_container_width=True)
        # ë™ì  í•´ì„¤
        most_missing = miss[miss > 0].head(3)
        txt = ", ".join([f"{col} ({ratio:.1%})" for col, ratio in most_missing.items()])
        st.warning(f"ê²°ì¸¡ì¹˜ ìƒìœ„ ì»¬ëŸ¼: {txt}")
        if most_missing.iloc[0] > 0.5:
            st.caption(f"â€¢ '{most_missing.index[0]}'ëŠ” ê²°ì¸¡ì´ 50%ë¥¼ ë„˜ì–´ ì‚­ì œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        elif most_missing.iloc[0] > 0.1:
            st.caption("â€¢ ê²°ì¸¡ 10~50%: ëŒ€ì¹˜(imputation) ë˜ëŠ” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            st.caption("â€¢ ê²°ì¸¡ 10% ì´í•˜: í‰ê· /ì¤‘ì•™ê°’/ìµœë¹ˆê°’ ë“±ìœ¼ë¡œ ì†ì‰½ê²Œ ëŒ€ì¹˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    st.write("ğŸ” ì¤‘ë³µ í–‰:", dup_cnt)
    if dup_cnt > 0:
        st.caption(f"â€¢ ì¤‘ë³µ ë°ì´í„° {dup_cnt}ê±´: ë°ì´í„° ì •í•©ì„± í™•ë³´ë¥¼ ìœ„í•´ ì œê±° ê¶Œì¥")
    else:
        st.caption("â€¢ ì¤‘ë³µ ë°ì´í„° ì—†ìŒ")

def _stats_and_target_dist(df: pd.DataFrame, target: str):
    st.subheader("2ï¸âƒ£ ê¸°ìˆ í†µê³„ & íƒ€ê¹ƒ ë¶„í¬")
    st.info("ê¸°ìˆ í†µê³„ë¡œ ë°ì´í„°ì˜ ë¶„í¬/ì´ìƒê°’/ëŒ€í‘œê°’ì„ íŒŒì•…í•˜ê³ , íƒ€ê¹ƒì˜ ë¶„í¬ê°€ ë¹„ëŒ€ì¹­/ë¾°ì¡±í•œì§€ í™•ì¸í•©ë‹ˆë‹¤. ì˜ˆì¸¡ëª¨ë¸ ì„ íƒ(ì˜ˆ: ë¡œê·¸ë³€í™˜)ì—ë„ ì˜í–¥.")
    st.write(df.describe().T)
    col1, col2 = st.columns(2)
    with col1:
        fig, ax = plt.subplots()
        sns.histplot(df[target].dropna(), kde=True, ax=ax)
        ax.set_title(f"Target Distribution â€“ {target}")
        st.pyplot(fig, use_container_width=True)
    with col2:
        sk = stats.skew(df[target].dropna())
        kt = stats.kurtosis(df[target].dropna())
        st.write(f"Skewness: {sk:.2f} &nbsp;&nbsp; Kurtosis: {kt:.2f}")
        msgs = []
        if abs(sk) > 1:
            msgs.append("âš ï¸ íƒ€ê¹ƒì´ ë¹„ëŒ€ì¹­(skew>1). ë¡œê·¸/Box-Cox ë³€í™˜ ì¶”ì²œ.")
        if abs(kt) > 3:
            msgs.append("âš ï¸ ë¾°ì¡±í•œ ë¶„í¬(kurtosis>3). ì´ìƒì¹˜ê°€ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if not msgs:
            st.success("íƒ€ê¹ƒ ë¶„í¬ê°€ ë¹„êµì  ì •ê·œë¶„í¬ì— ê°€ê¹ìŠµë‹ˆë‹¤.")
        else:
            for m in msgs:
                st.warning(m)

def _categorical_eda(df: pd.DataFrame, target: str):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns
    if not len(cat_cols):
        return
    st.subheader("3ï¸âƒ£ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„")
    st.info("ë²”ì£¼í˜• ë³€ìˆ˜(ì˜ˆ: ì§€ì—­, ì¹´í…Œê³ ë¦¬)ë³„ë¡œ íƒ€ê¹ƒì˜ í‰ê· Â·ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. high-cardinality ë³€ìˆ˜ëŠ” ì—”ì½”ë”© ì‹œ ì£¼ì˜ í•„ìš”.")
    for col in cat_cols:
        st.write(f"##### {col} (ìœ ë‹ˆí¬: {df[col].nunique()})")
        vc = df[col].value_counts().head(30)
        fig, ax = plt.subplots()
        vc.plot.bar(ax=ax)
        ax.set_ylabel("Count")
        st.pyplot(fig, use_container_width=True)
        grp = df.groupby(col)[target].agg(['mean', 'count']).sort_values('mean')
        st.write("íƒ€ê¹ƒ í‰ê·  ìƒìœ„/í•˜ìœ„ (Top10):")
        st.dataframe(grp.head(10).style.background_gradient('Blues', subset=['mean']), use_container_width=True)
        # ë™ì  í•´ì„¤
        if grp['count'].max() > 0.5 * len(df):
            st.caption(f"â€¢ '{col}'ì˜ ì¼ë¶€ ê°’ì´ ë°ì´í„°ì˜ ì ˆë°˜ ì´ìƒì„ ì°¨ì§€í•©ë‹ˆë‹¤. ê· í˜• ë¶ˆê· í˜• ì—¬ë¶€ ì²´í¬ í•„ìš”.")
        if df[col].nunique() > 20:
            st.caption(f"â€¢ '{col}' ë³€ìˆ˜ëŠ” ê³ ìœ ê°’ì´ {df[col].nunique()}ê°œë¡œ ë§ì•„ ê³ ì°¨ì› ì›-í•«ì¸ì½”ë”©ì— ì£¼ì˜.")
        # Boxplot
        if df[col].nunique() < 20:
            fig, ax = plt.subplots()
            sns.boxplot(x=df[col], y=df[target], ax=ax)
            ax.set_title(f"{target} by {col}")
            st.pyplot(fig, use_container_width=True)

    hc_cols = [c for c in cat_cols if df[c].nunique() > 20]
    for col in hc_cols:
        st.write(f"High-cardinality ë³€ìˆ˜(Top 20): {col}")
        top_vals = df[col].value_counts().head(20)
        fig, ax = plt.subplots()
        top_vals.plot.bar(ax=ax)
        st.pyplot(fig, use_container_width=True)

def _outlier_overview(df: pd.DataFrame, target: str):
    num_cols = df.select_dtypes(include="number").columns
    if not num_cols.any():
        return
    st.subheader("4ï¸âƒ£ ì´ìƒì¹˜ íƒìƒ‰")
    st.info("ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤. ì´ìƒì¹˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜Â·ë¶ˆì•ˆì • ì›ì¸ì´ ë  ìˆ˜ ìˆì–´ ë°˜ë“œì‹œ ì²´í¬í•´ì•¼ í•©ë‹ˆë‹¤.")
    sample = sample_df(df[num_cols])
    # IQR
    outlier_ratio = {}
    for col in num_cols:
        q1, q3 = np.percentile(sample[col].dropna(), [25, 75])
        iqr = q3 - q1
        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        mask = (sample[col] < lower) | (sample[col] > upper)
        outlier_ratio[col] = mask.mean()
    sr = pd.Series(outlier_ratio).sort_values(ascending=False)
    fig, ax = plt.subplots(figsize=(6, 3))
    sr.plot.bar(ax=ax)
    ax.set_ylabel("IQR Outlier Ratio")
    st.pyplot(fig, use_container_width=True)
    # ë™ì  í•´ì„¤
    high_outlier = sr[sr > 0.05]
    if not high_outlier.empty:
        st.warning("ì´ìƒì¹˜ ë¹„ìœ¨ì´ 5%ë¥¼ ë„˜ëŠ” ë³€ìˆ˜: " +
            ', '.join([f"{col}({ratio:.1%})" for col, ratio in high_outlier.items()]))
    else:
        st.success("ëª¨ë“  ë³€ìˆ˜ì˜ ì´ìƒì¹˜(IQR ê¸°ì¤€)ê°€ 5% ë¯¸ë§Œì…ë‹ˆë‹¤.")

    # Z-score
    st.write("**Z-score ê¸°ì¤€ ì´ìƒì¹˜ íƒìƒ‰ (|z| > 3):**")
    zscore_outlier = ((np.abs(stats.zscore(sample)) > 3).sum(axis=0) / len(sample))
    st.write(zscore_outlier)
    z_excess = zscore_outlier[zscore_outlier > 0.05]
    if not z_excess.empty:
        st.caption("Z-score ê¸°ì¤€ 5% ì´ìƒ ì´ìƒì¹˜ ë³€ìˆ˜: " + ', '.join(z_excess.index))

    # Isolation Forest
    if IsolationForest is not None:
        st.write("**Isolation Forestë¡œ íƒì§€ëœ ì´ìƒì¹˜ ë¹„ìœ¨:**")
        iso = IsolationForest(n_estimators=50, contamination=0.01, random_state=42)
        y_pred = iso.fit_predict(sample)
        outlier_rate = (y_pred == -1).mean()
        st.write(f"{outlier_rate:.2%}")
        if outlier_rate > 0.05:
            st.caption("Isolation Forestë¡œ ì „ì²´ ì´ìƒì¹˜ê°€ 5%ë¥¼ ë„˜ìœ¼ë¯€ë¡œ robust loss/model ë˜ëŠ” ì´ìƒì¹˜ ì œê±°ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
    else:
        st.info("IsolationForest ì„¤ì¹˜: pip install scikit-learn")

    # LOF
    if LocalOutlierFactor is not None:
        st.write("**LOF(Neighbor ê¸°ë°˜)ë¡œ íƒì§€ëœ ì´ìƒì¹˜ ë¹„ìœ¨:**")
        try:
            lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)
            y_pred = lof.fit_predict(sample)
            outlier_rate = (y_pred == -1).mean()
            st.write(f"{outlier_rate:.2%}")
            if outlier_rate > 0.05:
                st.caption("LOFë¡œ ì „ì²´ ì´ìƒì¹˜ê°€ 5%ë¥¼ ë„˜ìœ¼ë¯€ë¡œ ë³€ìˆ˜ scaling ë˜ëŠ” ì´ìƒì¹˜ ì˜í–¥ ì™„í™” í•„ìš”.")
        except Exception:
            st.info("LOFëŠ” 20ê°œ ì´ìƒì˜ ìƒ˜í”Œì—ì„œë§Œ ë™ì‘í•©ë‹ˆë‹¤.")
    else:
        st.info("LOF ì„¤ì¹˜: pip install scikit-learn")

def _multicollinearity_vif(df: pd.DataFrame):
    num_df = df.select_dtypes(include="number").dropna()
    if vif is None or num_df.shape[1] < 2:
        return
    st.subheader("5ï¸âƒ£ ë‹¤ì¤‘ê³µì„ ì„±(VIF)")
    st.info("ì…ë ¥ í”¼ì²˜ë“¤ë¼ë¦¬ ë„ˆë¬´ ê°•í•œ ìƒê´€(ê³µì„ ì„±)ì´ ìˆìœ¼ë©´ íšŒê·€ê³„ìˆ˜ í•´ì„Â·ì¼ë°˜í™” ì„±ëŠ¥ì´ ë‚˜ë¹ ì§‘ë‹ˆë‹¤. VIF > 10 ë³€ìˆ˜ëŠ” ì£¼ì˜!")
    sample = sample_df(num_df)
    X = sample.values
    vif_vals = [vif(X, i) for i in range(X.shape[1])]
    res = pd.DataFrame({
        "feature": num_df.columns,
        "VIF": vif_vals
    }).sort_values("VIF", ascending=False)
    st.dataframe(res, use_container_width=True)
    high_vif = res[res["VIF"] > 10]
    if not high_vif.empty:
        st.warning(
            "ë‹¤ì¤‘ê³µì„ ì„±ì´ ê°•í•œ ë³€ìˆ˜(VIF>10): " +
            ', '.join(high_vif['feature'].tolist()) +
            ". í•´ë‹¹ ë³€ìˆ˜ë“¤ì€ íšŒê·€ í•´ì„ë ¥ ì €í•˜Â·ë¶„ì‚°íŒ½ì°½ ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ì œê±°/ì¶•ì†Œ ê¶Œì¥!"
        )
    else:
        st.success("ëª¨ë“  ë³€ìˆ˜ì˜ VIF < 10. ë‹¤ì¤‘ê³µì„ ì„± ìš°ë ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _correlation(df: pd.DataFrame, target: str):
    num_df = df.select_dtypes(include="number")
    if num_df.shape[1] < 2:
        return
    st.subheader("6ï¸âƒ£ ìƒê´€ í–‰ë ¬(Pearson)")
    st.info("ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°„ ìƒê´€ê´€ê³„ì™€ íƒ€ê¹ƒê³¼ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤. ìƒê´€ |r| > 0.8ì€ ë‹¤ì¤‘ê³µì„ ì„± ìš°ë ¤, ì˜ë¯¸ìˆëŠ” feature engineering íŒíŠ¸ë„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    corr = num_df.corr()
    fig, ax = plt.subplots(figsize=(min(12, 0.5 * corr.shape[1]), 8))
    sns.heatmap(corr, cmap="coolwarm", center=0, ax=ax)
    st.pyplot(fig, use_container_width=True)
    # multicollinearity hint
    high_pairs = [
        (i, j)
        for i in corr.columns
        for j in corr.columns
        if i != j and abs(corr.loc[i, j]) > 0.8
    ]
    if high_pairs:
        st.warning("â€¢ |r| > .8 ë³€ìˆ˜ìŒ: " + ", ".join([f"{a}-{b}" for a, b in high_pairs]))
    else:
        st.success("ëª¨ë“  ë³€ìˆ˜ ìŒì—ì„œ |r| > .8 ì´ìƒì¸ ê²½ìš°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _scatter_top(df: pd.DataFrame, target: str, top_k=5):
    num_df = df.select_dtypes(include="number").drop(columns=[target], errors="ignore")
    if num_df.empty:
        return
    corrs = num_df.corrwith(df[target]).abs().sort_values(ascending=False)
    top_cols = corrs.head(top_k).index
    st.subheader(f"7ï¸âƒ£ íƒ€ê¹ƒê³¼ ìƒê´€ ìƒìœ„ {top_k}ê°œ í”¼ì²˜")
    st.info("íƒ€ê¹ƒê³¼ ìƒê´€ê´€ê³„ê°€ ê°€ì¥ ë†’ì€ í”¼ì²˜ì™€ì˜ ê´€ê³„ë¥¼ ì‚°ì ë„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤. ë¹„ì„ í˜•ì„±/íŠ¹ì´ê°’ ë“± í•´ì„ì—ë„ ìœ ìš©í•©ë‹ˆë‹¤.")
    for col in top_cols:
        fig, ax = plt.subplots()
        sns.scatterplot(x=df[col], y=df[target], ax=ax)
        ax.set_title(f"{col} vs {target}  (|r|={corrs[col]:.2f})")
        st.pyplot(fig, use_container_width=True)
        # ë™ì  í•´ì„¤
        if corrs[col] > 0.8:
            st.caption(f"{col}ê³¼(ì™€) íƒ€ê¹ƒì€ ë§¤ìš° ê°•í•œ ì„ í˜• ìƒê´€ê´€ê³„(|r|={corrs[col]:.2f})ë¥¼ ë³´ì…ë‹ˆë‹¤.")
        elif corrs[col] > 0.5:
            st.caption(f"{col}ê³¼(ì™€) íƒ€ê¹ƒì€ ì¤‘ê°„ ì´ìƒì˜ ìƒê´€ê´€ê³„(|r|={corrs[col]:.2f})ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.caption(f"{col}ê³¼(ì™€) íƒ€ê¹ƒì˜ ì„ í˜• ìƒê´€ì€ ì•½í•©ë‹ˆë‹¤(|r|={corrs[col]:.2f}).")

def _full_profile(df: pd.DataFrame):
    with st.expander("ğŸ” ì „ì²´ Profiling Report (optional)", expanded=False):
        st.info("ì „ì²´ ë³€ìˆ˜Â·ê´€ê³„Â·ë¶„í¬ë¥¼ ì¼ê´„ ì§„ë‹¨í•©ë‹ˆë‹¤. í° ë°ì´í„°ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if ydata_profiling:
            profile = ydata_profiling.ProfileReport(df, title="Profiling", minimal=True)
            st.components.v1.html(profile.to_html(), height=800, scrolling=True)
        else:
            st.info("`pip install ydata-profiling` í›„ ì „ì²´ ë¦¬í¬íŠ¸ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def generate(df: pd.DataFrame, target: str):
    """Streamlit í™”ë©´ì— ì™„ì „ ìë™ EDA + ê° ê²°ê³¼ë³„ ë™ì  í•´ì„¤"""
    df = sample_df(df)
    _overview(df)
    _missing_and_duplicates(df)
    _stats_and_target_dist(df, target)
    _categorical_eda(df, target)
    _outlier_overview(df, target)
    _multicollinearity_vif(df)
    _correlation(df, target)
    _scatter_top(df, target)
    _full_profile(df)